# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BLNmllmKf6FuzGjB_3s-MxNSAPOamuLX
"""

!pip install transformers

#word_embedding_model = models.BERT('bert-base-multilingual-cased')
from transformers import BertTokenizer, BertModel, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')
input_ids = torch.tensor(tokenizer.encode("ഉപകരണങ്ങൾ എവിടേയും", add_special_tokens=True)).unsqueeze(0) 
#input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
#"ഉപകരണങ്ങൾ വെബിൽ എവിടേയും നിങ്ങൾ തിരഞ്ഞെടുക്കുന്ന ഭാഷയിൽ ടൈപ്പുചെയ്യുന്നതിനെ സുഗമമാക്കുന്നു."
outputs = model(input_ids, masked_lm_labels=input_ids)

print ((outputs[1][0][0]))

from transformers import BertTokenizer, BertModel, BertForMaskedLM
import torch
token_vecs_sum = []
word_vectors=[]
text="ഇന്ത്യ പാക്കിസ്ഥാൻ രാജ്യവും  രാജ്യങ്ങളും സുഗമമാക്കുന്നു."
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')
splitted_text= text.split()
print (splitted_text)
complete_input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
print (complete_input_ids)
for word in splitted_text:
  input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)  # Batch size 1
  outputs = model(input_ids)
  last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
  # For each token in the sentence...
  for token in last_hidden_states:
    # `token` is a [12 x 768] tensor
    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)
    #token_vecs_sum.append(sum_vec)
  word_vectors.append(sum_vec)

#print (outputs[0][0][5])

sentencex=[]
sentencey=[]
sentencesx=[]
sentencesy=[]
new_predictions=[]

line_number=0
with open("final.txt", "r",encoding="utf8") as file:
    lines=file.readlines()
    list_of_tags='\\N_NN \\N_NNP \\N_NST \\PR_PRP \\PR_PRF \\PR_PRL \\PR_PRC \\PR_PRQ \\DM_DMD \\DM_DMR \\DM_DMQ \\V_VM \\V_VM_VF \\V_VM_VNF \\V_VM_VINF \\V_VN \\V_VAUX \\JJ \\RB \\PSP \\CC_CCD \\CC_CCS \\CC_CCS_UT \\RP_RPD \\RP_CL \\RP_INJ \\RP_INTF \\RP_NEG \\QT_QTF \\QT_QTC \\QT_QTO \\RD_RDF \\RD_SYM \\RD_PUNC \\RD_UNK \\RD_ECH'
    for line in lines:
        line_number=line_number+1
        
        #print (line)
        words=line.split("        ")
        #print (repr(words[1]))
        words[1]=words[1].rstrip('\t\n\r')
        if words[1] in list_of_tags:
                       
            if words[0]!=".":
                sentencex.append(words[0])
            
                
            if words[0]==".":
                sentencex.append(".")
                sentencesx.append(sentencex)
                sentencex=[]
            
            #print (line_number)
            #print (repr(words[1]))
            if words[1]!="\\RD_PUNC":
               sentencey.append(words[1])
            
            
            if words[1]=="\\RD_PUNC":
                sentencey.append("\\RDPUNC")
                sentencesy.append(sentencey)
                sentencey=[]
            if words[1]=="\\RD_PUNC" and words[0]!='.':
                print (line_number)
           
file.close()

print (len(sentencesy))
print (len(sentencesx))

#print (sentencesx[33])
import gensim
from gensim import corpora, models, similarities
from gensim.models import Word2Vec
count=0
#model=gensim.models.Word2Vec.load('word2vecmodel_size100_iter200')
model=Word2Vec.load("word2vecmodel_size100_iter200")
new_sentencesx=[]
for sentence in sentencesx:
  for word in sentence:
    try:
      new_sentencesx.append(model.wv[word])
    except:
      count=count+1
      new_sentencesx.append([0.0]*100)
new_sentencesy=[]
for sentence in sentencesy:
  for word in sentence:
    new_sentencesy.append(word)

from transformers import BertTokenizer, BertModel, BertForMaskedLM
import torch
new_sentencesx=[]
token_vecs_sum = []
word_vectors=[]
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')
#splitted_text= text.split()
#print (splitted_text)
#complete_input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
#print (complete_input_ids)
for sentence in sentencesx:
  sentence_vec=[]
  for word in sentence:
    input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
    # For each token in the sentence...
    for token in last_hidden_states:
      # `token` is a [12 x 768] tensor
      # Sum the vectors from the last four layers.
      sum_vec = torch.sum(token[-4:], dim=0)
      #token_vecs_sum.append(sum_vec)
      # Use `sum_vec` to represent `token`.
      sum_vec=sum_vec.detach().numpy()
    sentence_vec.append(sum_vec)
  new_sentencesx.append(sentence_vec)

import pandas
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
# encode class values as integers
encoder = LabelEncoder()
encoder.fit(new_sentencesy)
encoded_Y = encoder.transform(new_sentencesy)
# convert integers to dummy variables (i.e. one hot encoded)
train_y = np_utils.to_categorical(encoded_Y)
print (train_y[33])

print (len(new_sentencesx[0]))

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import classification_report

x_train, x_test, y_train, y_test = train_test_split(train_x,train_y, test_size= 0.2, random_state=27)
nn =MLPClassifier(hidden_layer_sizes=(400,400,400), max_iter=10, alpha=0.0001,solver='sgd',learning_rate='adaptive',learning_rate_init=0.1, verbose=10,  random_state=21, tol=0.0000001) 
nn.fit(x_train,y_train)
y_pred = nn.predict(x_test)

print (accuracy_score(y_test, y_pred))
print (precision_recall_fscore_support(y_test, y_pred, average='micro'))
#target_names = ['number', 'datenum', 'event', 'location', 'organization', 'name', 'things', 'other', 'occupation']
#print(classification_report(y_test, y_pred, target_names=target_names))

# define baseline model
def baseline_model():
	# create model
	model = Sequential()
	model.add(Dense(500, input_dim=768, activation='relu'))
	model.add(Dense(8, activation='softmax'))
	# Compile model
	model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
	return model
estimator = KerasClassifier(build_fn=baseline_model, epochs=1, batch_size=100, verbose=2)
kfold = KFold(n_splits=10, shuffle=True)
results = cross_val_score(estimator, new_sentencesx, dummy_y, cv=kfold)
print("Baseline: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

token_vecs_cat = []
token_vecs_sum = []

# `token_embeddings` is a [22 x 12 x 768] tensor.
"""
# For each token in the sentence...
for token in last_hidden_states:
    
    # `token` is a [12 x 768] tensor

    # Concatenate the vectors (that is, append them together) from the last 
    # four layers.
    # Each layer vector is 768 values, so `cat_vec` is length 3,072.
    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)
    
    # Use `cat_vec` to represent `token`.
    token_vecs_cat.append(cat_vec)
"""
# For each token in the sentence...
for token in last_hidden_states:

    # `token` is a [12 x 768] tensor

    # Sum the vectors from the last four layers.
    sum_vec = torch.sum(token[-4:], dim=0)
    
    # Use `sum_vec` to represent `token`.
    #token_vecs_sum.append(sum_vec)

!pip install git+https://www.github.com/keras-team/keras-contrib.git
!pip install sklearn_crfsuite

import numpy as np
from keras.models import load_model
import keras.preprocessing.text
from keras import optimizers
#from keras.layers import Dense, Merge
from keras.models import Model
from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge
from keras.layers.wrappers import TimeDistributed
from keras import optimizers
from keras.models import Sequential
from keras.layers.core import Dense, Activation, RepeatVector
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM
import os
import re
import codecs
from keras.layers.wrappers import TimeDistributed ,Bidirectional
import gensim
from gensim import corpora, models, similarities
from keras.preprocessing.sequence import pad_sequences
from nltk import FreqDist
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt
from keras.models import model_from_json
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from keras_contrib.utils import save_load_utils
#from keras.utils import plot_model
#from keras.layers import ChainCRF
#from crf import CRFLayer
import pandas as pd
from sklearn_crfsuite import metrics
from sklearn.preprocessing import MultiLabelBinarizer
from keras.callbacks import ModelCheckpoint
from keras_contrib.layers import CRF
from keras import backend as K
K.clear_session()
#print(keras.__version__)
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

#program pos tagging using word2vec and bilstm crf
from gensim.models import Word2Vec
i=0
j=1
k=1
l=-1
sentencex=[]
sentencey=[]
sentencesx=[]
sentencesy=[]
x=[]
y=[]

vocab_size=200000

with open("final.txt", "r",encoding="utf8") as file:
    lines=file.readlines()
    line_number=0
    for line in lines:
        line_number=line_number+1
        #print (line)
        list_of_tags='\\N_NN \\N_NNP \\N_NST \\PR_PRP \\PR_PRF \\PR_PRL \\PR_PRC \\PR_PRQ \\DM_DMD \\DM_DMR \\DM_DMQ \\V_VM \\V_VM_VF \\V_VM_VNF \\V_VM_VINF \\V_VN \\V_VAUX \\JJ \\RB \\PSP \\CC_CCD \\CC_CCS \\CC_CCS_UT \\RP_RPD \\RP_CL \\RP_INJ \\RP_INTF \\RP_NEG \\QT_QTF \\QT_QTC \\QT_QTO \\RD_RDF \\RD_SYM \\RD_PUNC \\RD_UNK \\RD_ECH'
        words=line.split()
        #print (repr(words[1]))
        
        if len(words)>1: 
        #and line_number<102507:
            words[1]=words[1].rstrip('\t\n\r')
            
            if words[0]!=".":
                sentencex.append(words[0])
                sentencey.append(words[1])            
                
            if words[0]=="." :
                sentencex.append(".")
                sentencey.append("\\RDPUNC")
                if len(sentencex)<=25:
                    sentencesx.append(sentencex)
                    sentencesy.append(sentencey)
                sentencex=[]
                sentencey=[]
    #print (len(sentencesx))
file.close() 

# Creating the vocabulary set with the most common words
dist = FreqDist(np.hstack(sentencesx))
X_vocab = dist.most_common(vocab_size-1)
dist = FreqDist(np.hstack(sentencesy))
Y_vocab = dist.most_common(vocab_size-1)

# Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary
X_ix_to_word = [word[0] for word in X_vocab]
# Adding the word "ZERO" to the beginning of the array
X_ix_to_word.insert(0, 'ZERO')
# Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)
X_ix_to_word.append('UNK')

#max_vocabullary=len(X_vocab)
# Creating the word-to-index dictionary from the array created above
#X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}

# Create the word-to-index dictionary from the array created above
Y_ix_to_word = [word[0] for word in Y_vocab]
Y_ix_to_word.insert(0, 'ZERO')
Y_ix_to_word.append('UNK')
Y_word_to_ix={word:ix for ix, word in enumerate(Y_ix_to_word)}

#Finding the maximum length sentence in the text.
max_len = max([len(sentence) for sentence in sentencesx])

def LoadRules(rules_file):
    print ("Loading the rules...")
    rules_dict = dict()
    line = []
    line_number = 0
    rule_number = 0
    rules_file = codecs.open(rules_file, encoding='utf-8', \
            errors='ignore')
    while 1:
        line_number = line_number +1
        text = rules_file.readline()
        if text == "":
            break
        if text[0] == '#':
            continue  #this is a comment - ignore
        text = text.split("#")[0]   #remove the comment part of the line
        line_number = line_number +1
        line = text.strip()  # remove unwanted space
        if(line == ""):
            continue
        if(len(line.split("=")) != 2):
            print ("[Error] Syntax Error in the Rules. Line number: ", \
                line_number)
            print ("Line: "+ text)
            continue
        lhs = line.split("=")[0].strip()
        rhs = line.split("=")[1].strip()
        if(len(rhs)>0):
            if(lhs[0] == '"'):
                lhs = lhs[1:len(lhs)] # if the string is "quoted"
            if(lhs[len(lhs)-1] == '"'):
                lhs = lhs[0:len(lhs)-1] # if the string is "quoted"
        if(len(rhs)>0):
            if(rhs[0] == '"'):
                rhs = rhs[1:len(rhs)]  # if the string is "quoted"
            if(rhs[len(rhs)-1] == '"'):
                rhs = rhs[0:len(rhs)-1]     # if the string is "quoted"
        rule_number = rule_number+1
        rules_dict[lhs] = rhs
        #print ("[", rule_number ,"] " +lhs + " : " +rhs)
    #print ("Found ",rule_number, " rules.")
    return rules_dict

def stemmer(word,rules_file,rules_dict):   
    flag=0
    #word = ""
    #pdb.set_trace()
    #word = unicode(word, errors='ignore')
    #word = unicode(word, "utf-8", errors='ignore')
    #word=word.decode("utf-8")
    if flag==0:
        #word = word.strip('!,.?:')
        word_length = len(word)
        #print word_length
        suffix_pos_itr = 2
        word_stemmed = ""
        #print "kakakaka"
        while suffix_pos_itr < word_length:
            suffix = word[suffix_pos_itr:word_length]
            suffix = re.sub('[\r\n]', '', suffix)
            #print repr(suffix)                     
            if suffix in rules_dict: 
                word_stemmed = rules_dict[suffix] 
                #print word_stemmed
                flag=1              
                break
            suffix_pos_itr = suffix_pos_itr+1
            
    if flag==0:
        
        suffix_pos_itr = 5
        suffix = word[(word_length-suffix_pos_itr):word_length]
        suffix = re.sub('[\r\n]', '', suffix)
        word_stemmed =suffix
        
    return word_stemmed
             
#rules_file = os.path.dirname(os.path.realpath('stemmer_ml_new.rules'))            
rules_file = os.path.join(os.path.dirname('__file__'), 'stemmer_ml_new.rules')
rules_dict=LoadRules(rules_file)    
           
model1 = Word2Vec.load("word2vecmodel_size100_iter200")
model2 = Word2Vec.load("word2vecmodel_size60_iter200_stem")

#taking stems and then finding vector of words
stemmd_sentences=[]
for sentence in sentencesx:
    sentenc=[]
    for word in sentence:
        suffix=stemmer(word,rules_file,rules_dict)
        sentenc.append(suffix)
    stemmd_sentences.append(sentenc)

k=model1.wv['രാജ്യം']          
m=model2.wv['്രശംസ']
# Converting each word to its word2vec value
#model2.train(stemmd_sentences,total_examples=model.corpus_count,epochs=model.iter) 
i=-1     
count=0
prev_word='നമ്മുടെ'
for sentence,sentence_stem in zip(sentencesx,stemmd_sentences):
    i=i+1    
    j=-1
    for wordx in sentence:
        j=j+1
        try:
            #s=np.concatenate((model1.wv[sentence[j]],model2.wv[sentence_stem[j]]),axis=0)
            s=model1.wv[sentence[j]]
            sentencesx[i][j]=s
        except (KeyError):
            count=count+1
            #print(model1.predict_output_word([prev_word,sentence[j+1]]))
            #s=np.concatenate((k,m),axis=0)
            s=k
            sentencesx[i][j]=s
        prev_word=wordx
            

y_ix_to_word = [word[0] for word in Y_vocab]
y_ix_to_word.insert(0, 'ZERO')
y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}
#print (y_word_to_ix)

for i, sentence in enumerate(sentencesy):
        for j, word in enumerate(sentence):
            if word in y_word_to_ix:
                sentencesy[i][j] = y_word_to_ix[word]
            else:
                print ("Hii")
                #sentencesy[i][j] = y_word_to_ix['UNK']

# Padding zeros to make all sequences have a same length with the longest one
print('[INFO] Zero padding...')
X = pad_sequences(sentencesx,padding='post', maxlen=max_len, dtype='int32')
y = pad_sequences(sentencesy,padding='post', maxlen=max_len, dtype='int32')
#print (repr(X[0]))
#print (repr(y[0]))

max_vocabullary=len(X_vocab)
output_classes=len(Y_vocab)
#print (max_vocabullary)
#print (output_classes)

#converting y sequences to one hot encoded form
y_sequences = np.zeros((len(y), max_len, len(y_word_to_ix)))
for i, sentence in enumerate(y):
    for j, word in enumerate(sentence):
        y_sequences[i, j, word] = 1.
#print (y_sequences[0])
        
train_x=X[:100000]
train_y=y_sequences[:100000]

x_train, x_test, y_train, y_test = train_test_split(train_x,train_y, test_size= 0.2, random_state=27)

#BIDIRECTIONAL LSTM MODEL IMPLIMENTATION
model = Sequential()
model.add(Bidirectional(LSTM(300, return_sequences=True), input_shape=(25, 100)))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(300, return_sequences=True)))
model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(150, activation='relu')))
model.add(TimeDistributed(Dense(37, activation='relu')))
#crf = CRFLayer()
#crf = ChainCRF()
crf = CRF(37)
model.add(crf)
#loss = crf.loss,
#model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ["accuracy"])
model.compile(optimizer="adam", loss=crf.loss_function, metrics=[crf.accuracy])
#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
model.summary()
# train LSTM
filename = 'BIDIRECTIONAL_LSTM_CRF_POS_TAGGING.h5'
checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')
history=model.fit(x_train, y_train, epochs=7, validation_data=[x_test,y_test], batch_size=100, callbacks=[checkpoint], verbose=2)

#program doing bert based pos tagging using bilstm crf
from transformers import BertTokenizer, BertModel, BertForMaskedLM
import torch
from gensim.models import Word2Vec
i=0
j=1
k=1
l=-1
sentencex=[]
sentencey=[]
sentencesx=[]
sentencesy=[]
x=[]
y=[]

vocab_size=200000

with open("final.txt", "r",encoding="utf8") as file:
    lines=file.readlines()
    line_number=0
    for line in lines:
        line_number=line_number+1
        #print (line)
        list_of_tags='\\N_NN \\N_NNP \\N_NST \\PR_PRP \\PR_PRF \\PR_PRL \\PR_PRC \\PR_PRQ \\DM_DMD \\DM_DMR \\DM_DMQ \\V_VM \\V_VM_VF \\V_VM_VNF \\V_VM_VINF \\V_VN \\V_VAUX \\JJ \\RB \\PSP \\CC_CCD \\CC_CCS \\CC_CCS_UT \\RP_RPD \\RP_CL \\RP_INJ \\RP_INTF \\RP_NEG \\QT_QTF \\QT_QTC \\QT_QTO \\RD_RDF \\RD_SYM \\RD_PUNC \\RD_UNK \\RD_ECH'
        words=line.split()
        #print (repr(words[1]))
        
        if len(words)>1: 
            words[1]=words[1].rstrip('\t\n\r')
            
            if words[0]!=".":
                sentencex.append(words[0])
                sentencey.append(words[1])            
                
            if words[0]=="." :
                sentencex.append(".")
                sentencey.append("\\RDPUNC")
                if len(sentencex)<=25:
                    sentencesx.append(sentencex)
                    sentencesy.append(sentencey)
                sentencex=[]
                sentencey=[]
file.close() 

# Creating the vocabulary set with the most common words
dist = FreqDist(np.hstack(sentencesx))
X_vocab = dist.most_common(vocab_size-1)
dist = FreqDist(np.hstack(sentencesy))
Y_vocab = dist.most_common(vocab_size-1)

# Creating an array of words from the vocabulary set, we will use this array as index-to-word dictionary
X_ix_to_word = [word[0] for word in X_vocab]
# Adding the word "ZERO" to the beginning of the array
X_ix_to_word.insert(0, 'ZERO')
# Adding the word 'UNK' to the end of the array (stands for UNKNOWN words)
X_ix_to_word.append('UNK')

#max_vocabullary=len(X_vocab)
# Creating the word-to-index dictionary from the array created above
#X_word_to_ix = {word:ix for ix, word in enumerate(X_ix_to_word)}

# Create the word-to-index dictionary from the array created above
Y_ix_to_word = [word[0] for word in Y_vocab]
Y_ix_to_word.insert(0, 'ZERO')
Y_ix_to_word.append('UNK')
Y_word_to_ix={word:ix for ix, word in enumerate(Y_ix_to_word)}

#Finding the maximum length sentence in the text.
max_len = max([len(sentence) for sentence in sentencesx])

def LoadRules(rules_file):
    print ("Loading the rules...")
    rules_dict = dict()
    line = []
    line_number = 0
    rule_number = 0
    rules_file = codecs.open(rules_file, encoding='utf-8', \
            errors='ignore')
    while 1:
        line_number = line_number +1
        text = rules_file.readline()
        if text == "":
            break
        if text[0] == '#':
            continue  #this is a comment - ignore
        text = text.split("#")[0]   #remove the comment part of the line
        line_number = line_number +1
        line = text.strip()  # remove unwanted space
        if(line == ""):
            continue
        if(len(line.split("=")) != 2):
            print ("[Error] Syntax Error in the Rules. Line number: ", \
                line_number)
            print ("Line: "+ text)
            continue
        lhs = line.split("=")[0].strip()
        rhs = line.split("=")[1].strip()
        if(len(rhs)>0):
            if(lhs[0] == '"'):
                lhs = lhs[1:len(lhs)] # if the string is "quoted"
            if(lhs[len(lhs)-1] == '"'):
                lhs = lhs[0:len(lhs)-1] # if the string is "quoted"
        if(len(rhs)>0):
            if(rhs[0] == '"'):
                rhs = rhs[1:len(rhs)]  # if the string is "quoted"
            if(rhs[len(rhs)-1] == '"'):
                rhs = rhs[0:len(rhs)-1]     # if the string is "quoted"
        rule_number = rule_number+1
        rules_dict[lhs] = rhs
        #print ("[", rule_number ,"] " +lhs + " : " +rhs)
    #print ("Found ",rule_number, " rules.")
    return rules_dict

def stemmer(word,rules_file,rules_dict):   
    flag=0
    #word = ""
    #pdb.set_trace()
    #word = unicode(word, errors='ignore')
    #word = unicode(word, "utf-8", errors='ignore')
    #word=word.decode("utf-8")
    if flag==0:
        #word = word.strip('!,.?:')
        word_length = len(word)
        #print word_length
        suffix_pos_itr = 2
        word_stemmed = ""
        #print "kakakaka"
        while suffix_pos_itr < word_length:
            suffix = word[suffix_pos_itr:word_length]
            suffix = re.sub('[\r\n]', '', suffix)
            #print repr(suffix)                     
            if suffix in rules_dict: 
                word_stemmed = rules_dict[suffix] 
                #print word_stemmed
                flag=1              
                break
            suffix_pos_itr = suffix_pos_itr+1
            
    if flag==0:
        
        suffix_pos_itr = 5
        suffix = word[(word_length-suffix_pos_itr):word_length]
        suffix = re.sub('[\r\n]', '', suffix)
        word_stemmed =suffix
        
    return word_stemmed
             
#rules_file = os.path.dirname(os.path.realpath('stemmer_ml_new.rules'))            
rules_file = os.path.join(os.path.dirname('__file__'), 'stemmer_ml_new.rules')
rules_dict=LoadRules(rules_file)    
           
model2 = Word2Vec.load("word2vecmodel_size60_iter200_stem")
#taking stems and then finding vector of words
stemmd_sentences=[]
for sentence in sentencesx:
    sentenc=[]
    for word in sentence:
        suffix=stemmer(word,rules_file,rules_dict)
        sentenc.append(suffix)
    stemmd_sentences.append(sentenc)
             
m=model2.wv['്രശംസ']

print ("Loading BERT embeddings")
new_sentencesx=[]
token_vecs_sum = []
word_vectors=[]
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained('bert-base-multilingual-cased')
for sentence,sentence_stem in zip(sentencesx,stemmd_sentences):
  j=-1
  sentence_vec=[]
  for word in sentence:
    j=j+1
    input_ids = torch.tensor(tokenizer.encode(word)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)
    last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
    # For each token in the sentence...
    for token in last_hidden_states:
      # `token` is a [12 x 768] tensor
      # Sum the vectors from the last four layers.
      sum_vec = torch.sum(token[-4:], dim=0)
      #token_vecs_sum.append(sum_vec)
      # Use `sum_vec` to represent `token`.
      sum_vec=sum_vec.detach().numpy()
    try:
      s=sum_vec
      #s=np.concatenate((sum_vec,model2.wv[sentence_stem[j]]),axis=0)
    except (KeyError):
      s=sum_vec
      #s=np.concatenate((sum_vec,m),axis=0)
    sentence_vec.append(s)
  new_sentencesx.append(sentence_vec)

y_ix_to_word = [word[0] for word in Y_vocab]
y_ix_to_word.insert(0, 'ZERO')
y_word_to_ix = {word:ix for ix, word in enumerate(y_ix_to_word)}
#print (y_word_to_ix)

for i, sentence in enumerate(sentencesy):
        for j, word in enumerate(sentence):
            if word in y_word_to_ix:
                sentencesy[i][j] = y_word_to_ix[word]
            else:
                print ("Hii")
                #sentencesy[i][j] = y_word_to_ix['UNK']

# Padding zeros to make all sequences have a same length with the longest one
print('[INFO] Zero padding...')
X = pad_sequences(new_sentencesx,padding='post', maxlen=max_len, dtype='int32')
y = pad_sequences(sentencesy,padding='post', maxlen=max_len, dtype='int32')
#print (repr(X[0]))
#print (repr(y[0]))

max_vocabullary=len(X_vocab)
output_classes=len(Y_vocab)

#converting y sequences to one hot encoded form
y_sequences = np.zeros((len(y), max_len, len(y_word_to_ix)))
for i, sentence in enumerate(y):
    for j, word in enumerate(sentence):
        y_sequences[i, j, word] = 1.
        
train_x=X[:100000]
train_y=y_sequences[:100000]

x_train, x_test, y_train, y_test = train_test_split(train_x,train_y, test_size= 0.2, random_state=27)

print ("MODEL CREATION")
#BIDIRECTIONAL LSTM MODEL IMPLIMENTATION
model = Sequential()
model.add(Bidirectional(LSTM(300, return_sequences=True), input_shape=(25, 768)))
model.add(Dropout(0.5))
model.add(Bidirectional(LSTM(300, return_sequences=True)))
model.add(Dropout(0.5))
model.add(TimeDistributed(Dense(150, activation='relu')))
model.add(TimeDistributed(Dense(37, activation='relu')))
crf = CRF(37)
model.add(crf)
model.compile(optimizer="adam", loss=crf.loss_function, metrics=[crf.accuracy])
model.summary()
filename = 'BIDIRECTIONAL_LSTM_CRF_POS_TAGGING_BERT.h5'
checkpoint = ModelCheckpoint(filename, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')
print ("MODEL FITTING")
history=model.fit(x_train, y_train, epochs=10, validation_data=[x_test,y_test], batch_size=100, callbacks=[checkpoint], verbose=2)
model.save("BIDIRECTIONAL_LSTM_CRF_POS_TAGGING_BERT.h5")

from keras.models import load_model
from sklearn.metrics import precision_recall_fscore_support
print ("Loading the model...")
filename='BIDIRECTIONAL_LSTM_CRF_POS_TAGGING_BERT.h5'
save_load_utils.load_all_weights(model,filename)
score = model.evaluate(x_test, y_test, verbose=2)
print (score)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

# Instantiate the MLB to turn string classes into one hot
y_ix_to_word_dict = {v: k for k, v in y_word_to_ix.items()}
mlb = MultiLabelBinarizer()
classes = [[item] for item in set(y_ix_to_word_dict.values())]
mlb.fit(classes)
classes = mlb.classes_
classes = pd.Series(classes)

Y_Test_OHE = y_test.copy()
y_test = np.argmax(y_test, axis=2)
# Turn the Y_Test_Named into class labels (Strings)
classes_dict = pd.DataFrame(classes).to_dict()[0]
Y_Test_Named = np.copy(y_test).astype(str)
for k, v in classes_dict.items(): Y_Test_Named[y_test==k] = v
    
labels=list(classes_dict.values())
labels= ['ZERO', '\\N_NN', '\\V_VM_VNF', '\\RDPUNC', '\\JJ', '\\V_VM_VF', '\\N_NNP', '\\V_VAUX', '\\RB', '\\QT_QTF', '\\DM_DMD', '\\PSP', '\\QT_QTO', '\\QT_QTC', '\\PR_PRP', '\\CC_CCD', '\\CC_CCS', '\\N_NST', '\\RP_INTF', '\\V_VM_VINF', '\\RD_RDF', '\\PR_PRF', '\\RP_NEG', '\\DM_DMR', '\\DM_DMQ', '\\RP_RPD', '\\PR_PRC', '\\V_VM', '\\V_VN', '\\RP_CL', '\\RP_INJ', '\\PR_PRL', '\\PR_PRQ', '\\RD_SYM', '\\CC_CCS_UT', '\\RD_ECH', '\\RD_UNK']
labels.remove('ZERO')
#sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))

y_Pred = model.predict_classes(x_test, verbose=0)
y_Pred2 = np.copy(y_Pred).astype(str)
for k, v in classes_dict.items():
    y_Pred2[y_Pred==k] = v

print (metrics.flat_classification_report(Y_Test_Named, y_Pred2, labels=labels, digits=3))

from keras.models import load_model
from sklearn.metrics import precision_recall_fscore_support
print ("Loading the model...")
#filename='BIDIRECTIONAL_LSTM_CRF_POS_TAGGING.h5'
#save_load_utils.load_all_weights(model,filename)
score = model.evaluate(x_test, y_test, verbose=2)
print (score)
print("%s: %.2f%%" % (model.metrics_names[1], score[1]*100))

# Instantiate the MLB to turn string classes into one hot
y_ix_to_word_dict = {v: k for k, v in y_word_to_ix.items()}
mlb = MultiLabelBinarizer()
classes = [[item] for item in set(y_ix_to_word_dict.values())]
mlb.fit(classes)
classes = mlb.classes_
classes = pd.Series(classes)

Y_Test_OHE = y_test.copy()
y_test = np.argmax(y_test, axis=2)
# Turn the Y_Test_Named into class labels (Strings)
classes_dict = pd.DataFrame(classes).to_dict()[0]
Y_Test_Named = np.copy(y_test).astype(str)
for k, v in classes_dict.items(): Y_Test_Named[y_test==k] = v
    
labels=list(classes_dict.values())
labels= ['ZERO', '\\N_NN', '\\V_VM_VNF', '\\RDPUNC', '\\JJ', '\\V_VM_VF', '\\N_NNP', '\\V_VAUX', '\\RB', '\\QT_QTF', '\\DM_DMD', '\\PSP', '\\QT_QTO', '\\QT_QTC', '\\PR_PRP', '\\CC_CCD', '\\CC_CCS', '\\N_NST', '\\RP_INTF', '\\V_VM_VINF', '\\RD_RDF', '\\PR_PRF', '\\RP_NEG', '\\DM_DMR', '\\DM_DMQ', '\\RP_RPD', '\\PR_PRC', '\\V_VM', '\\V_VN', '\\RP_CL', '\\RP_INJ', '\\PR_PRL', '\\PR_PRQ', '\\RD_SYM', '\\CC_CCS_UT', '\\RD_ECH', '\\RD_UNK']
labels.remove('ZERO')
#sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0]))

y_Pred = model.predict_classes(x_test, verbose=0)
y_Pred2 = np.copy(y_Pred).astype(str)
for k, v in classes_dict.items():
    y_Pred2[y_Pred==k] = v

print (metrics.flat_classification_report(Y_Test_Named, y_Pred2, labels=labels, digits=3))

